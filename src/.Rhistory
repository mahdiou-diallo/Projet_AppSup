E(net)$width <- 0
plot(net)
E(net)$width <- .1
plot(net)
E(net)$width <- 1+E(net)$weight/12
plot(net)
# Add a legend
legend(x = -1.5, y = 1.1, c("Newspaper", "Television", "Online News"),
pch = 21, col = "#777777", pt.bg = colors,
pt.cex = 1.8, cex = .9, bty = "n", ncol = 1)
# Finally
plot(net, vertex.label = V(net)$media)
legend(x = -1.5, y = 1.1, c("Newspaper", "Television", "Online News"),
pch = 21, col = "#777777", pt.bg = colors,
pt.cex = 1.8, cex = .8, bty = "n", ncol = 1)
ends(net, es = E(net), names = F)
ends(net, es = E(net), names = F)[,1]
# Add color edges based on their source node
edge.start <- ends(net, es = E(net), names = F)[,1]
edge.col <- V(net)$color[edge.start]
plot(net, edge.color = edge.col, vertex.label = V(net)$media)
# Add color edges based on their source node
edge.start <- ends(net, es = E(net), names = F)[,2]
edge.col <- V(net)$color[edge.start]
plot(net, edge.color = edge.col, vertex.label = V(net)$media)
# Add color edges based on their source node
edge.start <- ends(net, es = E(net), names = F)[,1]
edge.col <- V(net)$color[edge.start]
plot(net, edge.color = edge.col, vertex.label = V(net)$media)
legend(x = -1.5, y = 1.1, c("Newspaper", "Television", "Online News"),
pch = 21, col = "#777777", pt.bg = colors,
pt.cex = 1.8, cex = .8, bty = "n", ncol = 1)
# If semantic network, plot only labels
plot(net, edge.color = edge.col, vertex.label = V(net)$media,
vertex.shape = "none")
install.packages('miic')
data
data()
library(glmnet)
set.seed(42)
n <- 1000
p <- 5000
real.p <- 15
X <- matrix(rnorm(n*p), nrow = n, ncol=p)
y <- apply(X[,1:real.p], 1, sum) + rnorm(n)
train_rows <- sample(1:n, .66*n)
X.train <- X[train_rows,]
y.train <- y[train_rows]
X.test <- X[-train_rows,]
y.test <- y[-train_rows]
## Ridge Regression
alpha0.fit <- cv.glmnet(X.train, y.train,
type.measure = 'mse', alpha=0, family='gaussian')
alpha0.pred <- predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=X.test)
mean((alpha0.pred - y.test)^2)
set.seed(42)
n <- 1000
p <- 5000
real.p <- 15
X <- matrix(rnorm(n*p), nrow = n, ncol=p)
y <- apply(X[,1:real.p], 1, sum) + rnorm(n)
train_rows <- sample(1:n, .66*n)
X.train <- X[train_rows,]
y.train <- y[train_rows]
X.test <- X[-train_rows,]
y.test <- y[-train_rows]
## Ridge Regression
alpha0.fit <- cv.glmnet(X.train, y.train,
type.measure = 'mse', alpha=0, family='gaussian')
alpha0.pred <- predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=X.test)
mean((alpha0.pred - y.test)^2)
alpha1.fit <- cv.glmnet(X.train, y.train,
type.measure = 'mse', alpha=1, family='gaussian')
alpha1.pred <- predict(alpha0.fit, s=alpha1.fit$lambda.1se, newx=X.test)
mean((alpha1.pred - y.test)^2)
alpha1.pred <- predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=X.test)
mean((alpha1.pred - y.test)^2)
## Elastic Net Regressioin
alpha0.5.fit <- cv.glmnet(X.train, y.train,
type.measure = 'mse', alpha=.5, family='gaussian')
alpha0.5.pred <- predict(alpha0.5.fit, s=alpha0.5.fit$lambda.1se, newx=X.test)
mean((alpha0.5.pred - y.test)^2)
## Trying several alpha values
n.fits <- 10
fit.names <- sapply(0:n.fits, function(i) paste0('alpha', i/n.fits))
fit.names
fits <- sapply(0:n.fits, function(i) cv.glmnet(X.train, y.train,
type.measure = 'mse', alpha=i/n.fits, family='gaussian'))
head(fits)
fits
preds <- sapply(0:n.fits, function(i){
predict(fits[i], s=fits[i]$lambda.1se, newx=X.test)
})
class(fits)
fits[1]
fits[[1]]
preds <- sapply(0:n.fits, function(i){
predict(fits[[i]], s=fits[[i]]$lambda.1se, newx=X.test)
})
fits
class(fits)
predict(fits[,i], s=fits[,i]$lambda.1se, newx=X.test)
preds <- sapply(0:n.fits, function(i){
predict(fits[,i], s=fits[,i]$lambda.1se, newx=X.test)
})
preds <- sapply(0:n.fits, function(i) predict(fits[,i], s=fits[,i]$lambda.1se, newx=X.test))
fits[,i]
fits[,1]
preds <- sapply(0:n.fits, function(i) predict(fits[,i], s=fits[,i]$lambda.1se, newx=X.test))
class(fits[,1])
class(fits[1])
class(fits[[1]])
fits[[1]]
class(unlist(fits[,1]))
class(unclass(fits[,1]))
class(alpha0.5.fit)
lapply(1:3, print)
sapply(1:3, print)
preds <- sapply(0:n.fits, function(i) predict(fits[,i]$glmnet.fit, s=fits[,i]$lambda.1se, newx=X.test))
class(fits[,1][[1]])
class(fits[,1])
preds <- sapply(0:n.fits, function(i) predict(fits[,i+1]$glmnet.fit, s=fits[,i+1]$lambda.1se, newx=X.test))
preds <- sapply(0:n.fits, function(i) predict(fits[,i+1], s=fits[,i+1]$lambda.1se, newx=X.test))
preds <- sapply(0:n.fits, function(i) predict(fits[,i+1]$glmnet.fit, s=fits[,i+1]$lambda.1se, newx=X.test))
class(preds)
dim(preds)
mse <- sapply(0:n.fits, function(i){
mean(y.test, preds[,i+1])
})
mse <- sapply(0:n.fits, function(i) mean(y.test, preds[,i+1]))
mean(y.test, preds[,1])
preds[,1]
class(preds[,1])
class(y.test)
length(y.test)
mse <- sapply(0:n.fits, function(i) mean(y.test - preds[,i+1]))
results <- data.frame(alpha=(0:n.fits)/n.fits, mse=mse, fit.name=fit.names)
results
mse <- sapply(0:n.fits, function(i) mean((y.test - preds[,i+1])^2))
results <- data.frame(alpha=(0:n.fits)/n.fits, mse=mse, fit.name=fit.names)
results
plot(results[, 1:2])
plot(results[, 1:2], lty='b')
plot(results[, 1:2], type='b')
alpha0.fit$nzero
?cv.glmnet
alpha1.fit$nzero
plot(alpha1.fit$nzero)
plot(alpha1.fit$nzero, type = 'b', pch=16)
plot(alpha1.fit$nzero, type = 'b')
plot(1:length(alpha1.fit$nzero), alpha1.fit$nzero, type = 'b', pch=16)
plot(1:length(alpha1.fit$nzero), alpha1.fit$nzero, type = 'l', pch=16)
plot(1:length(alpha1.fit$nzero), alpha1.fit$nzero, type = 'b', pch=16)
1:length(alpha1.fit$nzero)-1
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16)
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main='number of non-zero coefficients vs $\lambda$',
xlab='$\lambda$', ylab='# used dimensions')
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main=paste0('number of non-zero coefficients vs ', lambda),
xlab=lambda, ylab='# used dimensions')
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main=expression(paste0('number of non-zero coefficients vs ', lambda)),
xlab=expression(lambda), ylab='# used dimensions')
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main=expression('number of non-zero coefficients vs ' lambda),
xlab=expression(lambda), ylab='# used dimensions')
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main=expression('number of non-zero coefficients vs ' lambda),
xlab=expression(lambda), ylab='# used dimensions')
plot(1:length(alpha1.fit$nzero)-1, alpha1.fit$nzero, pch=16,
main=expression('number of non-zero coefficients vs ' ~ lambda),
xlab=expression(lambda), ylab='# used dimensions')
install.packages('plspm')
library(plspm)
data("spainfoot")
summary(spainfoot)
?spainfoot
spainfoot
?`plspm-package`
# Load the package plspm
library(plspm)
# Load data spainfoot
data("spainfoot")
head(spainfoot)
?spainfoot
# INNER MODEL
# ------
# Rows of the inner model matrix
# Format: square and lower triangle boolean matrix
Attack = c(0, 0, 0)
Defense = c(0, 0, 0)
Success = c(1, 1, 0)
foot_path = rbind(Attack, Defense, Success)
colnames(foot_path) =  rownames(foot_path)
foot_path
# Nice plot
innerplot(foot_path)
# OUTER MODEL
# ------
# Define the list of indicators for each block
# Attack, Defense, Success
foot_blocks = list(1:4, 5:8, 9:12)
foot_blocks
# REFLECTIVE or FORMATIVE
# ------
# By default: mode A -- REFLECTIVE
foot_modes = c("A", "A", "A")
# RUN PLS-PM analysis
# ------
foot_pls = plspm(Data = spainfoot, path_matrix = foot_path,
blocks = foot_blocks, modes = foot_modes)
foot_pls
class(foot_pls)
# Path quantification
foot_pls$path_coefs
# Coefficient significance level
foot_pls$inner_model
summary(foot_pls)
# Plotting the analysis results
# ------
# Inner model
plot(foot_pls)
# Loadings
plot(foot_pls, what = "loadings", arr.width = 0.1)
# Scores for the index of success
head(foot_pls$scores, n = 5)
tail(foot_pls$scores, n = 5)
foot_pls$scores
# ------------------------
# Analyzing the results, assessing the measurement model and assessing the structural model
# ------------------------
foot_pls
summary(foot_pls)
# ----
# Unidimensionality of the indicators (3 possible metrics)
# ----
# Cronbach's alpha
# Dillon-Goldstein's rho
# The first eigenvalue of the indicators' correlation matrix
foot_pls$unidim
# Cronbach's alpha ~ average inter-variable correlation
# ----
# !! The computation of this metric requires the observed variables
# to be standardized and positively correlated
# Cronbach's index requires all indicators in reflective blocks to be positively correlated
foot_pls$unidim[, 3, drop = FALSE]
foot_pls$unidim
# First eigenvalue ~ eigen analysis of the correlation matrix of indicators
# ----
foot_pls$unidim[, 5:6]
# Solving the 'Defense' issue
# ----
# Plot loadings
plot(foot_pls, what = "loadings")
foot_pls$outer_model
subset(foot_pls$outer_model, block == "Defense")
# Plot weights
plot(foot_pls, what = "weights")
# add two more columns NGCH and NGCA
spainfoot$NGCH = -1 * spainfoot$GCH
spainfoot$NGCA = -1 * spainfoot$GCA
# rerun plspm
new_blocks_pos = list(1:4, c(15, 16, 7, 8), 9:12)
new_blocks_str = list(
c("GSH", "GSA", "SSH", "SSA"),
c("NGCH", "NGCA", "CSH", "CSA"),
c("WMH", "WMA", "LWR", "LRWL"))
foot_pls = plspm(spainfoot, foot_path, new_blocks_str, modes = foot_modes)
plot(foot_pls, "loadings")
# better results!
foot_pls$unidim
# ----
# Loadings and communalities
# ----
# loadings: correlations between latent variable and its indicators
# loadings ~ the amount of variance shared between the contruct and its indicators
# communalities: squared correlations
foot_pls$outer_model
# ----
# Cross-Loadings
# ----
# Loadings of an indicator with the rest of latent variables
foot_pls$crossloadings
# Structural model assessment
# ----
# Inspect the result of each regression
foot_pls$inner_model
# Coefficients of determination R² (for endogenous latent variables)
# ----
foot_pls$inner_summary
foot_pls$inner_summary[, "R2", drop = FALSE]
# Redundancy
# ----
# Reflects the ability of a set of independant latent variables to explain variation
# in the dependant latent variables
# High redundancy ==> high ability to predict
foot_pls$inner_summary
# better results!
foot_pls$unidim
foot_pls = plspm(spainfoot, foot_path, new_blocks_str, modes = foot_modes)
plot(foot_pls, "loadings")
# better results!
foot_pls$unidim
foot_pls = plspm(spainfoot, foot_path, new_blocks_str, modes = foot_modes)
plot(foot_pls, "loadings")
# better results!
foot_pls$unidim
# INNER MODEL
# ------
# Rows of the inner model matrix
# Format: square and lower triangle boolean matrix
Attack = c(0, 1, 1)
Defense = c(0, 0, 0)
Success = c(0, 0, 0)
foot_path = rbind(Attack, Defense, Success)
colnames(foot_path) =  rownames(foot_path)
foot_path
# Nice plot
innerplot(foot_path)
# OUTER MODEL
# ------
# Define the list of indicators for each block
# Attack, Defense, Success
foot_blocks = list(1:4, 5:8, 9:12)
# REFLECTIVE or FORMATIVE
# ------
# By default: mode A -- REFLECTIVE
foot_modes = c("A", "A", "A")
# RUN PLS-PM analysis
# ------
foot_pls = plspm(Data = spainfoot, path_matrix = foot_path,
blocks = foot_blocks, modes = foot_modes)
foot_pls
class(foot_pls)
# Path quantification
foot_pls$path_coefs
# INNER MODEL
# ------
# Rows of the inner model matrix
# Format: square and lower triangle boolean matrix
Attack = c(0, 1, 1)
Defense = c(0, 0, 0)
Success = c(0, 0, 0)
foot_path = rbind(Defense, Success, Attack)
colnames(foot_path) =  rownames(foot_path)
foot_path
# INNER MODEL
# ------
# Rows of the inner model matrix
# Format: square and lower triangle boolean matrix
Defense = c(0, 0, 0)
Success = c(0, 0, 0)
Attack = c(1, 1, 0)
foot_path = rbind(Defense, Success, Attack)
colnames(foot_path) =  rownames(foot_path)
foot_path
# Nice plot
innerplot(foot_path)
# OUTER MODEL
# ------
# Define the list of indicators for each block
# Attack, Defense, Success
foot_blocks = list(5:8, 9:12, 1:4)
# REFLECTIVE or FORMATIVE
# ------
# By default: mode A -- REFLECTIVE
foot_modes = c("A", "A", "A")
# RUN PLS-PM analysis
# ------
foot_pls = plspm(Data = spainfoot, path_matrix = foot_path,
blocks = foot_blocks, modes = foot_modes)
foot_pls
class(foot_pls)
# Path quantification
foot_pls$path_coefs
# Coefficient significance level
foot_pls$inner_model
summary(foot_pls)
# Plotting the analysis results
# ------
# Inner model
plot(foot_pls)
# Loadings
plot(foot_pls, what = "loadings", arr.width = 0.1)
# Scores for the index of success
head(foot_pls$scores, n = 5)
tail(foot_pls$scores, n = 5)
# ------------------------
# Analyzing the results, assessing the measurement model and assessing the structural model
# ------------------------
foot_pls
summary(foot_pls)
# ----
# Unidimensionality of the indicators (3 possible metrics) ----
#
# Cronbach's alpha
# Dillon-Goldstein's rho
# The first eigenvalue of the indicators' correlation matrix
foot_pls$unidim
# Cronbach's alpha ~ average inter-variable correlation
# ----
# !! The computation of this metric requires the observed variables
# to be standardized and positively correlated
# Cronbach's index requires all indicators in reflective blocks to be positively correlated
foot_pls$unidim[, 3, drop = FALSE]
# Dillon-Goldstein's rho ~ variance of the sum of variables ----
#
# Takes into account to which extend the latent variable explains its block of indicators
foot_pls$unidim[, 4, drop = FALSE]
# First eigenvalue ~ eigen analysis of the correlation matrix of indicators
# ----
foot_pls$unidim[, 5:6]
# Plot loadings
plot(foot_pls, what = "loadings")
foot_pls$outer_model
subset(foot_pls$outer_model, block == "Defense")
# Plot weights
plot(foot_pls, what = "weights")
# add two more columns NGCH and NGCA
spainfoot$NGCH = -1 * spainfoot$GCH
spainfoot$NGCA = -1 * spainfoot$GCA
# rerun plspm ----
new_blocks_pos = list(1:4, c(15, 16, 7, 8), 9:12)
new_blocks_str = list(
c("GSH", "GSA", "SSH", "SSA"),
c("NGCH", "NGCA", "CSH", "CSA"),
c("WMH", "WMA", "LWR", "LRWL"))
# rerun plspm ----
new_blocks_pos = list(c(15, 16, 7, 8), 9:12, 1:4)
new_blocks_str = list(
c("NGCH", "NGCA", "CSH", "CSA"),
c("WMH", "WMA", "LWR", "LRWL"),
c("GSH", "GSA", "SSH", "SSA"))
foot_pls = plspm(spainfoot, foot_path, new_blocks_str, modes = foot_modes)
plot(foot_pls, "loadings")
# better results!
foot_pls$unidim
#
# Loadings and communalities ----
#
# loadings: correlations between latent variable and its indicators
# loadings ~ the amount of variance shared between the contruct and its indicators
# communalities: squared correlations
foot_pls$outer_model
# ----
# Cross-Loadings
# ----
# Loadings of an indicator with the rest of latent variables
foot_pls$crossloadings
# Structural model assessment
# ----
# Inspect the result of each regression
foot_pls$inner_model
# Coefficients of determination R² (for endogenous latent variables)
# ----
foot_pls$inner_summary
library(FactoMineR)
data("iris")
res = PCA(iris[,-5])
plot(res$ind$coord[,1:2])
plot(res$ind$coord[,1:2], col = iris$Species)
plot(res$ind$coord[,1:2], col = iris$Species, pch=19)
legend('topright', legend = levels(iris$Species), col = 1:3, cex = 0.8, pch = 1)
legend('topright', legend = levels(iris$Species), col = 1:3, cex = 0.8, pch = 19)
library(FactoMineR)
data(wine)
res <- MFA(wine, group=c(2,5,3,10,9,2), type=c("n",rep("s",5)),
ncp=5, name.group=c("orig","olf","vis","olfag","gust","ens"),
num.group.sup=c(1,6))
summary(res)
getwd()
df <- read_csv('../input/visa.txt')
library(FactoMineR)
library(tidyverse)
df <- read_csv('../input/visa.txt')
df <- read_csv('../input/VisaPremier.txt')
setwd('C:/Users/user/Documents/_Masters/Cours/M2/Apprentissage Supervisé/projet/src')
library(FactoMineR)
library(tidyverse)
df <- read_csv('../input/VisaPremier.txt')
n <- nrow(df)
p <- ncol(df)
str(df)
df <- read_csv('../input/visa.csv')
n <- nrow(df)
p <- ncol(df)
str(df)
?drop_na
df <- read_csv('../input/visa.csv')
df_n <- drop_na(df, cartevpr, sexer)
n <- nrow(df_n)
p <- ncol(df_n)
str(df_n)
y_idx <- which(colnames(df_n) == 'cartevp')
y_idx
afdm <- FAMD(df,ncp=ncol(df_n), sup.var = y_idx)
y_idx <- which(colnames(df_n) == 'cartevp')
leave <- which((colnames(df) == 'cartevpr') | ((colnames(df) == 'sexer')))
leave
df <- df[, -leave]
df_n <- drop_na(df)
n <- nrow(df_n)
p <- ncol(df_n)
str(df_n)
# df[, (p-4):p] <- lapply(df[, (p-4):p], factor)
summary(df_n[, (p-4):p])
y_idx <- which(colnames(df_n) == 'cartevp')
afdm <- FAMD(df,ncp=ncol(df_n), sup.var = y_idx)
View(df_n)
View(df)
?PCA
